---
title: "Isolation Forest for Anomaly Detection"
author: "Sandesh"
date: "2023-01-14"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
image:
  placement: 2
  caption: "Photo by [kazuend](https://unsplash.com/photos/19SC2oaVZW0)"
---



<div id="introduction" class="section level3">
<h3>Introduction</h3>
<p>According to the
<a href="https://en.wikipedia.org/wiki/Anomaly_detection">Wikipedia</a>, “Anomaly
detection (also referred to as outliers detection and sometimes as
novelty detection) is generally understood to be the identification of
rare items, events or observations which deviate significantly from the
majority of the data and do not conform to a well defined notion of
normal behavior.” So, basically we are identifying any weird events or
data points. Now, these anomalies could be good or bad depending upon
your problem. For instance, in fraud detection the anomalies are bad,
meaning you want to reduce as much as possible. However, if you are
trying to find a new star in the galaxy then the anomalies in the
astronomy image might be good for you.</p>
<p>The application of the anomaly can be found in many domains like
cyber-security, financial fraud, medicine, law-enforcement, network
traffic, fault detection etc. The common nature of the problem in all of
those domains are there will be very few instances of the anomalies and
most of the data are normal. It’s like finding a needle in the haystack.</p>
<p>The simple outliers analysis that is done using box-plot’s IQR range,
Z-test are lighter version of the outliers analysis. However, it is not
robust where the data goes beyond uni-variate. Therefore, there are
various machine learning algorithms are developed that are more robust
and efficient in finding the outliers/anomalies.</p>
<p>Anomaly detection can be supervised or un-supervised depending upon the
dataset and data labels. In general, unsupervised technique is more
common and relevant to the application. Below, we will talk about an
unsupervised machine learning algorithm known as Isolation Forest to
detect anomalies.</p>
</div>
<div id="isolation-forest" class="section level3">
<h3>Isolation Forest</h3>
<p>From the name, you might have guessed it should be somewhat similar to
the <a href="https://en.wikipedia.org/wiki/Random_forest">‘Random Forest’</a>,
which is true. Isolation Forest is the ensemble method which fits
numerous decision tree and aggregate the results from the each tree to
get the outcome. Even though the basic principle of Random Forest and
Isolation Forest are same, there are some subtle differences which I
will be discussing below. You should definitely have some basic
foundation knowledge of Random Forest to understand the comparison.</p>
<ol style="list-style-type: decimal">
<li><p>Data Subset:</p>
<p>In the random forest, the data is bootstrapped meaning the
sub-samples are selected with replacement. However, in the Isolation
Forest random data points are selected without replacement which
means the there will not be any duplication in the sub-sampled data
set. According to the original paper, the idle number of sample for
the each tress in the Isolation Forest is 256 (Liu et.al 2012)</p></li>
<li><p>Split Criteria</p>
<p>In the decision tree that is build under Random Forest, Gini
Impurity, Entropy or Information Gain criteria are used to find the
best features for splitting the nodes. However, in isolation forest
the nodes are split randomly. According to Liu et.al (2012),
“Partitions are generated by randomly selecting an attribute and
then randomly selecting a split value between the maximum and
minimum values of the selected attribute.”</p></li>
</ol>
<p>Now, let’s try to understand what does it means by isolation and how
does it work.</p>
<div id="the-idea-of-isolation" class="section level4">
<h4>The Idea of Isolation</h4>
<p>According to the original paper by Liu et.al (2012), isolation means
“separating an instance from the rest of the instances”. The main
intuition behind isolation is that, the anomalies will be isolated early
as compared to inliers. The collective distance(from root node to leaf
node of respective anomalies) of the random decision tree for anomalies
will be shorter. Let’s look at the below figure to understand it more.</p>
<div class="figure">
<img src="dt_isolation_forest.png" alt="" />
<p class="caption">dt_isolation_forest</p>
</div>
<p>From the diagram, let suppose it is one of the decision tree from our
isolation forest. Let P1 be the path distance for the anomaly and p2,p3
are for the inliers. The main intuition is that anomalies will be
isolated early or split early because they are far away from the normal
cluster. Therefore, the path distance will be shorted. Now, we average
the path distance from all of the trees for the respective data-points.</p>
</div>
<div id="algorithm-breakdown" class="section level4">
<h4>Algorithm Breakdown</h4>
<p>The anomaly detection using iforest can be broke down into two steps
i.e. training and evaluation. I will not got over nitty-gritty details
on the math but will try my best to make you understand the working
principles.</p>
<div id="training" class="section level5">
<h5>Training</h5>
<p>In this phase, we are building the decision tress from our data. Below
is the training algorithm that I extracted from original paper.</p>
<div class="figure">
<img src="train_algo.png" alt="" />
<p class="caption">train_algo</p>
</div>
<p>The inputs are the <span class="math inline">\(X\)</span> (data), <span class="math inline">\(t\)</span> (number of trees in forest), <span class="math inline">\(\psi\)</span>
(sub-sampling size). The <span class="math inline">\(\psi\)</span> is the number of random observation
taken from the original data <span class="math inline">\(X\)</span> while making the forest. Empirically,
it is found that 256 is the ideal number for a various anomaly detection
problems. In the loop part of algorithms, we are basically making the
decision tree with the sub-sampling size to make an forest.</p>
</div>
<div id="evaluation" class="section level5">
<h5>Evaluation</h5>
<p>In the evaluation stage, the path distance is calculated from the each
tree and aggregated. After, calculating the distance, at the end,
anomaly scores are calculated for the each observations. I am not going
to explains the details of the formula here but just stating for
reference only:</p>
<p><span class="math display">\[s(x,\psi) = 2^\frac{-E(h(x))}{c(\psi)}\]</span></p>
<p>Here, <span class="math inline">\(E(h(x))\)</span> is the average path of the decision tree/isolation
trees, <span class="math inline">\(c(\psi)\)</span> is the average path length of unsuccessful searches in
Binary Search Tree. <span class="math inline">\(c(\psi)\)</span> is use to normalize the scores.</p>
<p>The anomaly score, <span class="math inline">\(s\)</span> can be used to make the following assessment:</p>
<ul>
<li>If instances return <span class="math inline">\(s\)</span> very close to 1, then they are definitely
anomalies,</li>
<li>If instances have <span class="math inline">\(s\)</span> much smaller than 0.5, then they are quite
safe to be regarded as normal instances, and</li>
<li>If all the instances return <span class="math inline">\(s≈0.5\)</span>, then the entire sample does not
really have any distinct anomaly.</li>
</ul>
</div>
</div>
<div id="performance-as-compared-to-other-anomaly-detection-algorithm" class="section level4">
<h4>Performance as Compared to Other Anomaly Detection Algorithm</h4>
<p>There are various anomaly detection algorithms out there. Some of the
famous ones are one-class SVM, clustering based methods (K-Means),
density-based (KNN). Here, I am not comparing myself but re-iterating
the result achieved by the authors in the original paper. They compared
various open-sourced dataset with the algorithms such as ORCA (Bay
et.al. 2003), LOF (Breunig et,al. 2000), one-class SVM and Random
Forest.</p>
<p>Even though anomaly detection mostly is unsupervised learning (unlabeled
data) but for the purpose of comparison authors have used the labeled
data and calculated AUC score. From the below figure, you can see
iforest is achieving highest AUC score on most of the datasets. The <span class="math inline">\(**\)</span>
means out of memory.(The author used low powered computing processor for
comparison)</p>
<div class="figure">
<img src="performance_comp.png" alt="" />
<p class="caption">performance_comp</p>
</div>
<p>The performance of the algorithm might depend on one’s respective
problem and use case. However, IForest is not only popular for it’s
performance but also for the training and evaluation time. The Isolation
Forest is pretty fast as compared to other algorithms. The below figure
compares the time taken by different compared algorithm. The <span class="math inline">\(*\)</span> means
execution will take more then 2 weeks because of the larger dataset. The
top most dataset are larger dataset. The authors have concluded that the
algorithms performs efficient and fast even when the dataset are
extremely large. For reference, <span class="math inline">\(Http\)</span> dataset have 567,497 instances.</p>
<div class="figure">
<img src="proc_time.png" alt="" />
<p class="caption">proc_time</p>
</div>
</div>
<div id="python-code-demo-with-sklearn" class="section level4">
<h4>Python Code Demo with Sklearn</h4>
<p>In this section, I will demo how to implement the Isolation Forest in
Python using the Scikit-Learn (sklearn) Library. I am using a dummy
dataset that is also synthetically generated using sklearn.</p>
<pre class="python"><code>
# Libraries Import

from sklearn.datasets import make_blobs, make_moons
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest
from sklearn.metrics import accuracy_score
from sklearn import tree
from matplotlib import pyplot as plt
import warnings
warnings.filterwarnings(&#39;ignore&#39;)</code></pre>
<pre class="python"><code>
# Dummy dataset Generation
data = make_blobs(centers=[[0,0],[2,2]], 
                  cluster_std=0.4, 
                  n_samples=[350,10], 
                  n_features=2, 
                  random_state=112233)

# Features of the &#39;data&#39;
feat = pd.DataFrame(data[0], columns=[&#39;X1&#39;, &#39;X2&#39;])

# Target column of the &#39;data&#39;
target = pd.DataFrame(data[1], columns=[&#39;target&#39;])

# Combine Both to make a data-frame
df = pd.concat([feat, target], axis=1)

print(df[&#39;target&#39;].value_counts())</code></pre>
<pre><code>## 0    350
## 1     10
## Name: target, dtype: int64</code></pre>
<pre class="python"><code>&quot;&quot;&quot;There are total 360 data-points, out of which 350 are of class 0 and 10 of class 1. Also, there are 2 continuous features names as X1 and X2. As mentioned previously, even anomaly detection is majorly unsupervised, for the sake of comparision we are derriving as supervised learning&quot;&quot;&quot;</code></pre>
<pre><code>## &#39;There are total 360 data-points, out of which 350 are of class 0 and 10 of class 1. Also, there are 2 continuous features names as X1 and X2. As mentioned previously, even anomaly detection is majorly unsupervised, for the sake of comparision we are derriving as supervised learning&#39;</code></pre>
<pre class="python"><code>
# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(feat, target, train_size=0.7, 
                                                    random_state=112233, 
                                                    stratify=target, 
                                                    shuffle=True)</code></pre>
<pre class="python"><code>
# Visulize the Train &amp; Test data
fig, axs = plt.subplots(1,2)
axs[0].scatter(x=X_train[&#39;X1&#39;], y=X_train[&#39;X2&#39;], c=y_train[&#39;target&#39;],)
axs[0].set_title(&quot;Train Data&quot;)
axs[1].scatter(x=X_test[&#39;X1&#39;], y=X_test[&#39;X2&#39;], c=y_test[&#39;target&#39;])
axs[1].set_title(&quot;Test Data&quot;)
plt.show()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/Data%20Viz-1.png" width="672" /></p>
<pre class="python"><code>&quot;&quot;&quot;From the figure, it is clear that what data-points are anomaly. However, this
is just for test case. Yellow points (class 1) are out anomalies.&quot;&quot;&quot;</code></pre>
<pre><code>## &#39;From the figure, it is clear that what data-points are anomaly. However, this\nis just for test case. Yellow points (class 1) are out anomalies.&#39;</code></pre>
<pre class="python"><code>
# Fit Isolation Forest Model
&quot;&quot;&quot;By default, the model will have 100 tress and the subsample size is 256. 
There is another parameter call contamination rate, which is percentages of 
anomalies supposed to present in the dataset.&quot;&quot;&quot;</code></pre>
<pre><code>## &#39;By default, the model will have 100 tress and the subsample size is 256. \nThere is another parameter call contamination rate, which is percentages of \nanomalies supposed to present in the dataset.&#39;</code></pre>
<pre class="python"><code>mdl = IsolationForest(random_state=112233, contamination=0.02).fit(X_train)</code></pre>
<pre class="python"><code># Visualizing one of the Isolation Tree
#fig, ax = plt.subplots(1,1)
_ = tree.plot_tree(mdl.estimators_[0])
plt.show()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-3.png" width="672" /><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-1-4.png" width="672" /></p>
<p>The visual of the above figure is not that clear and that’s not what I
want you to notice. In theory, we mentioned that the anomalies will be
separated as leaf node with few split as compared to the in-liers. So,
if you look at the decision tree right hand part, there are bunch of
values that are separated with just few split however, majority of them,
on left side, took many splits. Those data-points that took few split
are anomalies. This is just for a single tree and remember we have 100.</p>
<pre class="python"><code>
# Prediction on the Test Data
pred = mdl.predict(X_test)

&quot;&quot;&quot;The Predict function outputs the values as 1 and -1. -1 being anomalies. 
We need to convert this back to 0 and 1. For us the class 1 are anomalies&quot;&quot;&quot;

# Decode to it original format
pred_decode = [1 if i==-1 else 0 for i in pred]

# Accuracy
print(f&quot;The accuracy is: {accuracy_score(y_test, pred_decode)}&quot;)

&quot;&quot;&quot;We got the accuracy of 100% on the test data, which is what I was expecting, as the data was easily sepearatable. 
However, the purpose of the demo is to understand how to use algorithm with sklearn. You will face complex situation in real-world&quot;&quot;&quot;</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/Evaluation-7.png" width="672" /></p>
</div>
<div id="additional-notes" class="section level4">
<h4>Additional Notes</h4>
<div id="contamination-rate" class="section level5">
<h5>Contamination Rate</h5>
<p>Contamination rate is the ratio of anomalies presented in the dataset.
We can get the value for contamination rate from our domain knowledge
depending upon the problem or by looking at the historic trend. The
contamination rate/level ranges between 0.02 and 0.5. If the
contamination rate is way higher like 0.5 and above, I think the problem
will change from anomalies detection to something like classification or
clustering. The accuracy and predictive performance depends upon the
contamination level so, it is crucial to identify relevant contamination
level before modeling.</p>
</div>
<div id="continuous-variable" class="section level5">
<h5>Continuous Variable</h5>
<p>In the original paper too, only continuous variables are used. We
definitely can do one-hot, ordinal and other types of encoding to
convert the categorical data to it’s respective numeric values. I am not
sure, how the algorithm performs when we have majority of categorical
features. That will be something to experiment with in future.</p>
<p>Finally, we are at the end, I hope this article gives your basic
intuition about the isolation forest and it’s working principles along
with some hands-on demo in Python. One will see problems of anomaly
detection widely in the real-world therefore, it is important to have
some tools and knowledge to tackle those problems. I would highly
recommend to read the original paper by Liu. et.al (2012), which
contains a through information on algorithms and also on comparison with
other anomaly detection methods.</p>
</div>
</div>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<p>Anomaly detection. (2023, January 2). In Wikipedia.
<a href="https://en.wikipedia.org/wiki/Anomaly_detection" class="uri">https://en.wikipedia.org/wiki/Anomaly_detection</a></p>
<div class="csl-entry">
Bay, S. D., &amp; Schwabacher, M. (2003). Mining Distance-Based Outliers in
Near Linear Time with Randomization and a Simple Pruning Rule.
<i>Proceedings of the Ninth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining</i>, 29–38.
</div>
<div class="csl-entry">
Breunig, M. M., Kriegel, H.-P., Ng, R. T., &amp; Sander, J. (2000). LOF:
Identifying Density-Based Local Outliers. <i> Proceedings of the 2000
ACM SIGMOD International Conference on Management of Data</i>, 93–104.
</div>
<div class="csl-entry">
Liu, F. T., Ting, K. M., &amp; Zhou, Z.-H. (2012). Isolation-based anomaly
detection. <i>ACM Trans. Knowl. Discov. Data</i>, <i>6</i>(3), 39.
<a href="https://doi.org/10.1145/2133360.2133363" class="uri">https://doi.org/10.1145/2133360.2133363</a>
</div>
</div>
