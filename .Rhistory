scores_CV_acc.append(lg_cv.score(X_testCV, y_testCV))
scores_test_acc.append(lg_cv.score(X_test, y_test))
scores_CV_f1.append(round(f1_score(y_testCV, lg_cv.predict(X_testCV)),2))
scores_test_f1.append(round(f1_score(y_test, lg_cv.predict(X_test)),2))
print(lg_cv.score(X_testCV, y_testCV))
for train_index, test_index in kf.split(X_train, y_train):
X_trainCV, X_testCV = X_train[train_index], X[test_index]
y_trainCV, y_testCV = y_train[train_index], y[test_index]
lg_cv = LogisticRegression(random_state=112233).fit(X_trainCV, y_trainCV)
scores_CV_acc.append(lg_cv.score(X_testCV, y_testCV))
scores_test_acc.append(lg_cv.score(X_test, y_test))
scores_CV_f1.append(round(f1_score(y_testCV, lg_cv.predict(X_testCV)),2))
scores_test_f1.append(round(f1_score(y_test, lg_cv.predict(X_test)),2))
print(lg_cv.score(X_testCV, y_testCV))
for i in scores_CV_f1:
print(i)
for i in scores_test_f1:
print(i)
len(scores_test_f1)
from sklearn.model_selection import StratifiedKFold
kf = StratifiedKFold(n_splits = 10, shuffle=True, random_state=112233)
#  Accuracy
scores_CV_acc = []
scores_test_acc = []
# F1 Score
scores_CV_f1 = []
scores_test_f1 = []
kf
kf.get_n_splits
kf.n_splits
from sklearn.model_selection import StratifiedKFold
kf = StratifiedKFold(n_splits = 10, shuffle=True, random_state=112233)
#  Accuracy
scores_CV_acc = []
scores_test_acc = []
# F1 Score
scores_CV_f1 = []
scores_test_f1 = []
for train_index, test_index in kf.split(X_train, y_train):
X_trainCV, X_testCV = X_train[train_index], X[test_index]
y_trainCV, y_testCV = y_train[train_index], y[test_index]
lg_cv = LogisticRegression(random_state=112233).fit(X_trainCV, y_trainCV)
scores_CV_acc.append(lg_cv.score(X_testCV, y_testCV))
scores_test_acc.append(lg_cv.score(X_test, y_test))
scores_CV_f1.append(round(f1_score(y_testCV, lg_cv.predict(X_testCV)),2))
scores_test_f1.append(round(f1_score(y_test, lg_cv.predict(X_test)),2))
print(lg_cv.score(X_testCV, y_testCV))
for i in scores_test_f1:
print(i)
View(scores_CV_acc)
scores_CV_acc[0]
gc()
reticulate::repl_python()
import pandas as pd
import numpy as np
from statsmodels.tsa.arima_process import ArmaProcess #for data generation
from statsmodels.tsa.stattools import adfuller #adf test
# Assigning random coefficient
ar1 = np.array([1, -0.9])
# Since we are generating AR1 data the MA part remains constant
ma1 = np.array([1])
ar_obj = ArmaProcess(ar1, ma1)
np.random.seed(112233)
# Simulate the Data
simulate_ar1 = ar_obj.generate_sample(nsample=50)
# Convert the data to pandas data-frame
sim_ar1 = pd.DataFrame({'data': simulate_ar1})
sim_ar1.head()
adf_test_ar1 = adfuller(sim_ar1['data'])
adf_test_ar1 = adfuller(sim_ar1['data'])
print(f"The t-stat is: {adf_test_ar1[0]}")
adf_test_ar1 = adfuller(sim_ar1['data'])
print(f"The t-stat is: {adf_test_ar1[0]}")
print(f"The p-value is: {adf_test_ar1[1]}")
adf_test_ar1 = adfuller(sim_ar1['data'])
print(f"The t-stat is: {adf_test_ar1[0]}")
print(f"The p-value is: {adf_test_ar1[1]}")
exit
f
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
reticulate::repl_python()
# Import Libraries and Data
from sklearn.datasets import load_diabetes
from sklearn.tree import DecisionTreeRegressor
from sklearn import metrics
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
# data and split
df= load_diabetes()
train_size = 300
x_train, y_train = df.data[:train_size], df.target[:train_size]
x_test, y_test = df.data[train_size:], df.target[train_size:]
np.random.seed(112233)
# Create the Pre-requisite for the Gradient Boost
# Number of Decision Tree
dt_size = 50
# Learning Rate
lr = 0.1
# Placeholder for all base learners
base_learner = []
# Initial Prediction (which is mean: Step 1)
prv_pred = np.zeros(len(y_train))+ np.mean(y_train)
# Loop (Step 2)
for i in range(dt_size):
# Pseudo Residuals
err = y_train - prv_pred
# Train decision tree on the pseudo residual
dt = DecisionTreeRegressor(max_depth=1, random_state=112233)
dt.fit(x_train, err)
# Prediction of the decision tree
gamma = dt.predict(x_train)
# Update the model
prv_pred = prv_pred + lr * gamma
# Save the base learner
base_learner.append(dt)
base_learner
# Make the prediction
# Again initializing the base learner prediction which is mean
pred0 = np.zeros(len(y_test))+ np.mean(y_test)
# Loop through the base learner to predict
for learner in base_learner:
prediction = learner.predict(x_test)
pred0 = pred0 + (lr * prediction)
pred0
# Evaluations using RMSE
rmse = metrics.mean_squared_error(y_test, pred0, squared=False)
print(f"RMSE: {rmse}")
# Now Using the Sklearn Package
gbr = GradientBoostingRegressor(n_estimators=dt_size, max_depth=1, learning_rate=lr, random_state=112233)
gbr.fit(x_train, y_train)
gbr_pred = gbr.predict(x_test)
gbr_err = metrics.mean_squared_error(y_test, gbr_pred, squared=False)
print(f"RMSE: {rmse}")
print(f"RMSE (sklearn) : {gbr_err}")
for i in range(2):
print(i)
for i in range(2):
print(i)
for i in range(2):
print(i);
for i in range(3):
print(i);
for i in range(3):
print(i);
for i in range(4):
print(i);
for i in range(4):
print(i);
for i in range(4):
print(i);
# Import Libraries and Data
from sklearn.datasets import load_diabetes
from sklearn.tree import DecisionTreeRegressor
from sklearn import metrics
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
# data and split
df= load_diabetes()
train_size = 300
x_train, y_train = df.data[:train_size], df.target[:train_size]
x_test, y_test = df.data[train_size:], df.target[train_size:]
np.random.seed(112233)
# Import Libraries and Data
from sklearn.datasets import load_diabetes
from sklearn.tree import DecisionTreeRegressor
from sklearn import metrics
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
# data and split
df= load_diabetes()
train_size = 300
x_train, y_train = df.data[:train_size], df.target[:train_size]
x_test, y_test = df.data[train_size:], df.target[train_size:]
np.random.seed(112233)
# Create the Pre-requisite for the Gradient Boost
# Number of Decision Tree
dt_size = 50
# Learning Rate
lr = 0.1
# Placeholder for all base learners
base_learner = []
# Initial Prediction (which is mean: Step 1)
prv_pred = np.zeros(len(y_train))+ np.mean(y_train)
# Loop (Step 2)
for i in range(dt_size):
# Pseudo Residuals
err = y_train - prv_pred
# Train decision tree on the pseudo residual
dt = DecisionTreeRegressor(max_depth=1, random_state=112233)
dt.fit(x_train, err)
# Prediction of the decision tree
gamma = dt.predict(x_train)
# Update the model
prv_pred = prv_pred + lr * gamma
# Save the base learner
base_learner.append(dt)
# Make the prediction
# Again initializing the base learner prediction which is mean
pred0 = np.zeros(len(y_test))+ np.mean(y_test)
# Loop through the base learner to predict
for learner in base_learner:
prediction = learner.predict(x_test)
pred0 = pred0 + (lr * prediction);
# Evaluations using RMSE
rmse = metrics.mean_squared_error(y_test, pred0, squared=False)
print(f"RMSE: {rmse}")
# Now Using the Sklearn Package
gbr = GradientBoostingRegressor(n_estimators=dt_size, max_depth=1, learning_rate=lr, random_state=112233)
gbr.fit(x_train, y_train)
gbr_pred = gbr.predict(x_test)
gbr_err = metrics.mean_squared_error(y_test, gbr_pred, squared=False)
print(f"RMSE: {rmse}")
print(f"RMSE (sklearn) : {gbr_err}")
# Loop (Step 2)
for i in range(dt_size):
# Pseudo Residuals
err = y_train - prv_pred
# Train decision tree on the pseudo residual
dt = DecisionTreeRegressor(max_depth=1, random_state=112233)
dt.fit(x_train, err)
# Prediction of the decision tree
gamma = dt.predict(x_train)
# Update the model
prv_pred = prv_pred + lr * gamma
# Save the base learner
base_learner.append(dt);
# Make the prediction
# Again initializing the base learner prediction which is mean
pred0 = np.zeros(len(y_test))+ np.mean(y_test)
# Loop through the base learner to predict
for learner in base_learner:
prediction = learner.predict(x_test)
pred0 = pred0 + (lr * prediction);
# Make the prediction
# Again initializing the base learner prediction which is mean
pred0 = np.zeros(len(y_test))+ np.mean(y_test)
# Loop through the base learner to predict
for learner in base_learner:
prediction = learner.predict(x_test)
pred0 = pred0 + (lr * prediction);
# Make the prediction
# Again initializing the base learner prediction which is mean
pred0 = np.zeros(len(y_test))+ np.mean(y_test)
# Loop through the base learner to predict
for learner in base_learner:
prediction = learner.predict(x_test)
pred0 = pred0 + (lr * prediction);
# Evaluations using RMSE
rmse = metrics.mean_squared_error(y_test, pred0, squared=False)
print(f"RMSE: {rmse}")
# Now Using the Sklearn Package
gbr = GradientBoostingRegressor(n_estimators=dt_size, max_depth=1, learning_rate=lr, random_state=112233)
gbr.fit(x_train, y_train)
gbr_pred = gbr.predict(x_test)
gbr_err = metrics.mean_squared_error(y_test, gbr_pred, squared=False)
print(f"RMSE: {rmse}")
print(f"RMSE (sklearn) : {gbr_err}")
for i in range(4):
print(i);
# Loop (Step 2)
for i in range(dt_size):
# Pseudo Residuals
err = y_train - prv_pred
# Train decision tree on the pseudo residual
dt = DecisionTreeRegressor(max_depth=1, random_state=112233)
dt.fit(x_train, err)
# Prediction of the decision tree
gamma = dt.predict(x_train)
# Update the model
prv_pred = prv_pred + lr * gamma
# Save the base learner
base_learner.append(dt);
# Loop (Step 2)
for i in range(dt_size):
# Pseudo Residuals
err = y_train - prv_pred
# Train decision tree on the pseudo residual
dt = DecisionTreeRegressor(max_depth=1, random_state=112233)
dt.fit(x_train, err)
# Prediction of the decision tree
gamma = dt.predict(x_train)
# Update the model
prv_pred = prv_pred + lr * gamma
# Save the base learner
base_learner.append(dt);
# Loop (Step 2)
for i in range(dt_size):
# Pseudo Residuals
err = y_train - prv_pred
# Train decision tree on the pseudo residual
dt = DecisionTreeRegressor(max_depth=1, random_state=112233)
dt.fit(x_train, err)
# Prediction of the decision tree
gamma = dt.predict(x_train)
# Update the model
prv_pred = prv_pred + lr * gamma
# Save the base learner
base_learner.append(dt);
# Make the prediction
# Again initializing the base learner prediction which is mean
pred0 = np.zeros(len(y_test))+ np.mean(y_test)
# Loop through the base learner to predict
for learner in base_learner:
prediction = learner.predict(x_test)
pred0 = pred0 + (lr * prediction);
# Evaluations using RMSE
rmse = metrics.mean_squared_error(y_test, pred0, squared=False)
print(f"RMSE: {rmse}")
# Now Using the Sklearn Package
gbr = GradientBoostingRegressor(n_estimators=dt_size, max_depth=1, learning_rate=lr, random_state=112233)
gbr.fit(x_train, y_train)
gbr_pred = gbr.predict(x_test)
gbr_err = metrics.mean_squared_error(y_test, gbr_pred, squared=False)
print(f"RMSE: {rmse}")
print(f"RMSE (sklearn) : {gbr_err}")
for i in range(4):
print(i);
# Make the prediction
# Again initializing the base learner prediction which is mean
pred0 = np.zeros(len(y_test))+ np.mean(y_test)
# Loop through the base learner to predict
for learner in base_learner:
prediction = learner.predict(x_test)
pred0 = pred0 + (lr * prediction);
# Loop (Step 2)
for i in range(dt_size):
# Pseudo Residuals
err = y_train - prv_pred
# Train decision tree on the pseudo residual
dt = DecisionTreeRegressor(max_depth=1, random_state=112233)
dt.fit(x_train, err)
# Prediction of the decision tree
gamma = dt.predict(x_train)
# Update the model
prv_pred = prv_pred + lr * gamma
# Save the base learner
base_learner.append(dt);
# Loop (Step 2)
for i in range(dt_size):
# Pseudo Residuals
err = y_train - prv_pred
# Train decision tree on the pseudo residual
dt = DecisionTreeRegressor(max_depth=1, random_state=112233)
dt.fit(x_train, err)
# Prediction of the decision tree
gamma = dt.predict(x_train)
# Update the model
prv_pred = prv_pred + lr * gamma
# Save the base learner
base_learner.append(dt)
# Loop (Step 2)
for i in range(dt_size):
# Pseudo Residuals
err = y_train - prv_pred
# Train decision tree on the pseudo residual
dt = DecisionTreeRegressor(max_depth=1, random_state=112233)
dt.fit(x_train, err)
# Prediction of the decision tree
gamma = dt.predict(x_train)
# Update the model
prv_pred = prv_pred + lr * gamma
# Save the base learner
base_learner.append(dt);
# Loop (Step 2)
for i in range(dt_size):
# Pseudo Residuals
err = y_train - prv_pred
# Train decision tree on the pseudo residual
dt = DecisionTreeRegressor(max_depth=1, random_state=112233)
dt.fit(x_train, err)
# Prediction of the decision tree
gamma = dt.predict(x_train)
# Update the model
prv_pred = prv_pred + lr * gamma
# Save the base learner
base_learner.append(dt);
for i in range(4):
print(i);
for i in range(4):
print(10)
for i in range(10):
print(10);
for i in range(10):
print(i);
for i in range(dt_size):
# Pseudo Residuals
err = y_train - prv_pred
# Train decision tree on the pseudo residual
dt = DecisionTreeRegressor(max_depth=1, random_state=112233)
dt.fit(x_train, err)
# Prediction of the decision tree
gamma = dt.predict(x_train)
# Update the model
prv_pred = prv_pred + lr * gamma
# Save the base learner
base_learner.append(dt);
base_learner
# Make the prediction
# Again initializing the base learner prediction which is mean
pred0 = np.zeros(len(y_test))+ np.mean(y_test)
# Loop through the base learner to predict
for learner in base_learner:
prediction = learner.predict(x_test)
pred0 = pred0 + (lr * prediction);
# Evaluations using RMSE
rmse = metrics.mean_squared_error(y_test, pred0, squared=False)
print(f"RMSE: {rmse}")
# Now Using the Sklearn Package
gbr = GradientBoostingRegressor(n_estimators=dt_size, max_depth=1, learning_rate=lr, random_state=112233)
gbr.fit(x_train, y_train)
gbr_pred = gbr.predict(x_test)
gbr_err = metrics.mean_squared_error(y_test, gbr_pred, squared=False)
print(f"RMSE: {rmse}")
print(f"RMSE (sklearn) : {gbr_err}")
reticulate::repl_python()
# Now Using the Sklearn Package for comparision
gbr = GradientBoostingRegressor(n_estimators=dt_size, max_depth=1, learning_rate=lr, random_state=112233)
gbr.fit(x_train, y_train)
gbr_pred = gbr.predict(x_test)
gbr_err = metrics.mean_squared_error(y_test, gbr_pred, squared=False)
print(f"RMSE: {rmse}")
print(f"RMSE (sklearn) : {gbr_err}")
# Import Libraries and Data
from sklearn.datasets import load_diabetes
from sklearn.tree import DecisionTreeRegressor
from sklearn import metrics
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
# data and split
df= load_diabetes()
train_size = 300
x_train, y_train = df.data[:train_size], df.target[:train_size]
x_test, y_test = df.data[train_size:], df.target[train_size:]
np.random.seed(112233)
# Create the Pre-requisite for the Gradient Boost
# Number of Decision Tree
dt_size = 50
# Learning Rate
lr = 0.1
# Placeholder for all base learners
base_learner = []
# Initial Prediction (which is mean: Step 1)
prv_pred = np.zeros(len(y_train))+ np.mean(y_train)
# Loop (Step 2)
for i in range(dt_size):
# Pseudo Residuals
err = y_train - prv_pred
# Train decision tree on the pseudo residual
dt = DecisionTreeRegressor(max_depth=1, random_state=112233)
dt.fit(x_train, err)
# Prediction of the decision tree
gamma = dt.predict(x_train)
# Update the model
prv_pred = prv_pred + lr * gamma
# Save the base learners
base_learner.append(dt);
# Loop (Step 2)
for i in range(dt_size):
# Pseudo Residuals
err = y_train - prv_pred
# Train decision tree on the pseudo residual
dt = DecisionTreeRegressor(max_depth=1, random_state=112233)
dt.fit(x_train, err)
# Prediction of the decision tree
gamma = dt.predict(x_train)
# Update the model
prv_pred = prv_pred + lr * gamma
# Save the base learners
base_learner.append(dt);
for i in range(dt_size):
# Pseudo Residuals
err = y_train - prv_pred
# Train decision tree on the pseudo residual
dt = DecisionTreeRegressor(max_depth=1, random_state=112233)
dt.fit(x_train, err)
# Prediction of the decision tree
gamma = dt.predict(x_train)
# Update the model
prv_pred = prv_pred + lr * gamma
# Save the base learners
base_learner.append(dt);
# Make the prediction
# Again initializing the base learner prediction which is mean
pred0 = np.zeros(len(y_test))+ np.mean(y_test)
# Loop through the learners to predict
for learner in base_learner:
prediction = learner.predict(x_test)
pred0 = pred0 + (lr * prediction);
# Evaluations using RMSE
rmse = metrics.mean_squared_error(y_test, pred0, squared=False)
print(f"RMSE: {rmse}")
# Now Using the Sklearn Package for comparision
gbr = GradientBoostingRegressor(n_estimators=dt_size, max_depth=1, learning_rate=lr, random_state=112233)
gbr.fit(x_train, y_train)
gbr_pred = gbr.predict(x_test)
gbr_err = metrics.mean_squared_error(y_test, gbr_pred, squared=False)
print(f"RMSE: {rmse}")
print(f"RMSE (sklearn) : {gbr_err}")
quit
quit
exit
print("this")
blogdown:::serve_site()
blogdown::build_site()
