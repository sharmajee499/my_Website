# Loop through the learners to predict
for learner in base_learner:
prediction = learner.predict(x_test)
pred0 = pred0 + (lr * prediction);
# Evaluations using RMSE
rmse = metrics.mean_squared_error(y_test, pred0, squared=False)
print(f"RMSE: {rmse}")
# Now Using the Sklearn Package for comparision
gbr = GradientBoostingRegressor(n_estimators=dt_size, max_depth=1, learning_rate=lr, random_state=112233)
gbr.fit(x_train, y_train)
gbr_pred = gbr.predict(x_test)
gbr_err = metrics.mean_squared_error(y_test, gbr_pred, squared=False)
print(f"RMSE: {rmse}")
print(f"RMSE (sklearn) : {gbr_err}")
quit
quit
exit
print("this")
blogdown:::serve_site()
blogdown::build_site()
reticulate::repl_python()
# Import the Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# Exponential Distribution or Farm 1 data-points
rng = np.random.default_rng(seed=112233)
exp_dist = rng.exponential(1, 10000)
sns.histplot(exp_dist, kde=True)
plt.title("Simulated Exponential Distribution")
# Exponential Distribution or Farm 1 data-points
rng = np.random.default_rng(seed=112233)
exp_dist = rng.exponential(1, 10000)
sns.histplot(exp_dist, kde=True)
plt.title("Simulated Exponential Distribution")
plt.show()
# Sample Mean Calculator function
def sample_mean(distribution_array, sample_size, n_samples):
# Initialized the list to store value
sample_mean = []
for i in range(n_samples):
# Choose the random samples (Sampling)
sample = np.random.choice(distribution_array, sample_size,)
# Calculate the mean of the drawn sample
mean = np.mean(sample)
# Append the result
sample_mean.append(mean)
return sample_mean
# Plots for different sample sizes
fig, axs = plt.subplots(2,2)
sns.histplot(ax=axs[0,0], x=sample_mean(exp_dist, 1, 500), kde=True)
axs[0,0].set_title("Sample Drawn: 1, 500 Times")
sns.histplot(ax=axs[0,1], x=sample_mean(exp_dist, 10, 500), kde=True)
axs[0,1].set_title("Sample Drawn: 10, 500 Times")
sns.histplot(ax=axs[1,0], x=sample_mean(exp_dist, 30, 500), kde=True)
axs[1,0].set_title("Sample Drawn: 30, 500 Times")
sns.histplot(ax=axs[1,1], x=sample_mean(exp_dist, 50, 500), kde=True)
axs[1,1].set_title("Sample Drawn: 50, 500 Times")
fig.tight_layout()
plt.show()
# Exponential Distribution or Farm 1 data-points
rng = np.random.default_rng(seed=112233)
exp_dist = rng.exponential(1, 10000)
sns.histplot(exp_dist, kde=True)
plt.title("Simulated Exponential Distribution")
plt.show()
# Import the Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# Exponential Distribution or Farm 1 data-points
rng = np.random.default_rng(seed=112233)
exp_dist = rng.exponential(1, 10000)
sns.histplot(exp_dist, kde=True)
plt.title("Simulated Exponential Distribution")
plt.show()
# Import the Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# Exponential Distribution or Farm 1 data-points
rng = np.random.default_rng(seed=112233)
exp_dist = rng.exponential(1, 10000)
sns.histplot(exp_dist, kde=True)
plt.title("Simulated Exponential Distribution")
plt.show()
# Import the Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# Exponential Distribution or Farm 1 data-points
rng = np.random.default_rng(seed=112233)
exp_dist = rng.exponential(1, 10000)
sns.histplot(exp_dist, kde=True)
plt.title("Simulated Exponential Distribution")
plt.show()
# Sample Mean Calculator function
def sample_mean(distribution_array, sample_size, n_samples):
# Initialized the list to store value
sample_mean = []
for i in range(n_samples):
# Choose the random samples (Sampling)
sample = np.random.choice(distribution_array, sample_size,)
# Calculate the mean of the drawn sample
mean = np.mean(sample)
# Append the result
sample_mean.append(mean)
return sample_mean
# Plots for different sample sizes
fig, axs = plt.subplots(2,2)
sns.histplot(ax=axs[0,0], x=sample_mean(exp_dist, 1, 500), kde=True)
axs[0,0].set_title("Sample Drawn: 1, 500 Times")
sns.histplot(ax=axs[0,1], x=sample_mean(exp_dist, 10, 500), kde=True)
axs[0,1].set_title("Sample Drawn: 10, 500 Times")
sns.histplot(ax=axs[1,0], x=sample_mean(exp_dist, 30, 500), kde=True)
axs[1,0].set_title("Sample Drawn: 30, 500 Times")
sns.histplot(ax=axs[1,1], x=sample_mean(exp_dist, 50, 500), kde=True)
axs[1,1].set_title("Sample Drawn: 50, 500 Times")
fig.tight_layout()
plt.show()
exit
reticulate::repl_python()
# Import the Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# Exponential Distribution or Farm 1 data-points
rng = np.random.default_rng(seed=112233)
exp_dist = rng.exponential(1, 10000)
sns.histplot(exp_dist, kde=True)
plt.title("Simulated Exponential Distribution")
plt.show()
# Plots for different sample sizes
fig, axs = plt.subplots(2,2)
sns.histplot(ax=axs[0,0], x=sample_mean(exp_dist, 1, 500), kde=True)
axs[0,0].set_title("Sample Drawn: 1, 500 Times")
sns.histplot(ax=axs[0,1], x=sample_mean(exp_dist, 10, 500), kde=True)
axs[0,1].set_title("Sample Drawn: 10, 500 Times")
sns.histplot(ax=axs[1,0], x=sample_mean(exp_dist, 30, 500), kde=True)
axs[1,0].set_title("Sample Drawn: 30, 500 Times")
sns.histplot(ax=axs[1,1], x=sample_mean(exp_dist, 50, 500), kde=True)
axs[1,1].set_title("Sample Drawn: 50, 500 Times")
fig.tight_layout()
fig.show()
# Sample Mean Calculator function
def sample_mean(distribution_array, sample_size, n_samples):
# Initialized the list to store value
sample_mean = []
for i in range(n_samples):
# Choose the random samples (Sampling)
sample = np.random.choice(distribution_array, sample_size,)
# Calculate the mean of the drawn sample
mean = np.mean(sample)
# Append the result
sample_mean.append(mean)
return sample_mean
# Plots for different sample sizes
fig, axs = plt.subplots(2,2)
sns.histplot(ax=axs[0,0], x=sample_mean(exp_dist, 1, 500), kde=True)
axs[0,0].set_title("Sample Drawn: 1, 500 Times")
sns.histplot(ax=axs[0,1], x=sample_mean(exp_dist, 10, 500), kde=True)
axs[0,1].set_title("Sample Drawn: 10, 500 Times")
sns.histplot(ax=axs[1,0], x=sample_mean(exp_dist, 30, 500), kde=True)
axs[1,0].set_title("Sample Drawn: 30, 500 Times")
sns.histplot(ax=axs[1,1], x=sample_mean(exp_dist, 50, 500), kde=True)
axs[1,1].set_title("Sample Drawn: 50, 500 Times")
fig.tight_layout()
fig.show()
# Plots for different sample sizes
fig, axs = plt.subplots(2,2)
sns.histplot(ax=axs[0,0], x=sample_mean(exp_dist, 1, 500), kde=True)
axs[0,0].set_title("Sample Drawn: 1, 500 Times")
sns.histplot(ax=axs[0,1], x=sample_mean(exp_dist, 10, 500), kde=True)
axs[0,1].set_title("Sample Drawn: 10, 500 Times")
sns.histplot(ax=axs[1,0], x=sample_mean(exp_dist, 30, 500), kde=True)
axs[1,0].set_title("Sample Drawn: 30, 500 Times")
sns.histplot(ax=axs[1,1], x=sample_mean(exp_dist, 50, 500), kde=True)
axs[1,1].set_title("Sample Drawn: 50, 500 Times")
fig.tight_layout()
# Plots for different sample sizes
fig, axs = plt1.subplots(2,2)
sns.histplot(ax=axs[0,0], x=sample_mean(exp_dist, 1, 500), kde=True)
axs[0,0].set_title("Sample Drawn: 1, 500 Times")
sns.histplot(ax=axs[0,1], x=sample_mean(exp_dist, 10, 500), kde=True)
axs[0,1].set_title("Sample Drawn: 10, 500 Times")
sns.histplot(ax=axs[1,0], x=sample_mean(exp_dist, 30, 500), kde=True)
axs[1,0].set_title("Sample Drawn: 30, 500 Times")
sns.histplot(ax=axs[1,1], x=sample_mean(exp_dist, 50, 500), kde=True)
axs[1,1].set_title("Sample Drawn: 50, 500 Times")
fig.tight_layout()
# Plots for different sample sizes
fig, axs = plt1.subplots(2,2)
sns.histplot(ax=axs[0,0], x=sample_mean(exp_dist, 1, 500), kde=True)
axs[0,0].set_title("Sample Drawn: 1, 500 Times")
sns.histplot(ax=axs[0,1], x=sample_mean(exp_dist, 10, 500), kde=True)
axs[0,1].set_title("Sample Drawn: 10, 500 Times")
sns.histplot(ax=axs[1,0], x=sample_mean(exp_dist, 30, 500), kde=True)
axs[1,0].set_title("Sample Drawn: 30, 500 Times")
sns.histplot(ax=axs[1,1], x=sample_mean(exp_dist, 50, 500), kde=True)
axs[1,1].set_title("Sample Drawn: 50, 500 Times")
fig.tight_layout()
plt1.show()
# Exponential Distribution or Farm 1 data-points
rng = np.random.default_rng(seed=112233)
exp_dist = rng.exponential(1, 10000)
sns.histplot(exp_dist, kde=True)
plt.title("Simulated Exponential Distribution")
# Exponential Distribution or Farm 1 data-points
rng = np.random.default_rng(seed=112233)
exp_dist = rng.exponential(1, 10000)
sns.histplot(exp_dist, kde=True)
plt.title("Simulated Exponential Distribution")
# Exponential Distribution or Farm 1 data-points
rng = np.random.default_rng(seed=112233)
exp_dist = rng.exponential(1, 10000)
sns.histplot(exp_dist, kde=True)
plt.title("Simulated Exponential Distribution")
plt.show()
quit()
quit
print("this")
read.csv()
reticulate::repl_python()
# Import the Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# Exponential Distribution or Farm 1 data-points
rng = np.random.default_rng(seed=112233)
exp_dist = rng.exponential(1, 10000)
sns.histplot(exp_dist, kde=True)
plt.title("Simulated Exponential Distribution")
plt.show()
# Exponential Distribution or Farm 1 data-points
rng = np.random.default_rng(seed=112233)
exp_dist = rng.exponential(1, 10000)
sns.histplot(exp_dist, kde=True)
plt.title("Simulated Exponential Distribution")
# Plots for different sample sizes
fig, axs = plt.subplots(2,2)
sns.histplot(ax=axs[0,0], x=sample_mean(exp_dist, 1, 500), kde=True)
axs[0,0].set_title("Sample Drawn: 1, 500 Times")
sns.histplot(ax=axs[0,1], x=sample_mean(exp_dist, 10, 500), kde=True)
axs[0,1].set_title("Sample Drawn: 10, 500 Times")
sns.histplot(ax=axs[1,0], x=sample_mean(exp_dist, 30, 500), kde=True)
axs[1,0].set_title("Sample Drawn: 30, 500 Times")
sns.histplot(ax=axs[1,1], x=sample_mean(exp_dist, 50, 500), kde=True)
axs[1,1].set_title("Sample Drawn: 50, 500 Times")
fig.tight_layout()
# Plots for different sample sizes
fig, axs = plt.subplots(2,2)
sns.histplot(ax=axs[0,0], x=sample_mean(exp_dist, 1, 500), kde=True)
axs[0,0].set_title("Sample Drawn: 1, 500 Times")
sns.histplot(ax=axs[0,1], x=sample_mean(exp_dist, 10, 500), kde=True)
axs[0,1].set_title("Sample Drawn: 10, 500 Times")
sns.histplot(ax=axs[1,0], x=sample_mean(exp_dist, 30, 500), kde=True)
axs[1,0].set_title("Sample Drawn: 30, 500 Times")
sns.histplot(ax=axs[1,1], x=sample_mean(exp_dist, 50, 500), kde=True)
axs[1,1].set_title("Sample Drawn: 50, 500 Times")
fig.tight_layout()
plt.show()
quit
blogdown:::serve_site()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
reticulate::repl_python()
# Libraries Import
from sklearn.datasets import make_blobs, make_moons
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest
from sklearn.metrics import accuracy_score
from sklearn import tree
from matplotlib import pyplot as plt
import warnings
warnings.filterwarnings('ignore')
# Dummy dataset Generation
data = make_blobs(centers=[[0,0],[2,2]], cluster_std=0.4, n_samples=[350,10], n_features=2, random_state=112233)
# Features of the 'data'
feat = pd.DataFrame(data[0], columns=['X1', 'X2'])
# Target column of the 'data'
target = pd.DataFrame(data[1], columns=['target'])
# Combine Both to make a data-frame
df = pd.concat([feat, target], axis=1)
print(df['target'].value_counts())
"""There are total 360 data-points, out of which 350 are of class 0 and 10 of class 1. Also, there are 2 continuous features names as X1 and X2. As mentioned previously, even anomaly detection is majorly unsupervised, for the sake of comparision we are derriving as supervised learning"""
# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(feat, target, train_size=0.7,
random_state=112233,
stratify=target,
shuffle=True)
X_train
# Visulize the Train & Test data
fig, axs = plt.subplots(1,2)
axs[0].scatter(x=X_train['X1'], y=X_train['X2'], c=y_train['target'],)
axs[0].set_title("Train Data")
axs[1].scatter(x=X_test['X1'], y=X_test['X2'], c=y_test['target'])
axs[1].set_title("Test Data")
plt.plot()
"""From the figure, it is clear that what data-points are anomaly. However, this is just for test case. Yellow points (class 1) are out anomalies."""
# Visulize the Train & Test data
fig, axs = plt.subplots(1,2)
axs[0].scatter(x=X_train['X1'], y=X_train['X2'], c=y_train['target'],)
axs[0].set_title("Train Data")
axs[1].scatter(x=X_test['X1'], y=X_test['X2'], c=y_test['target'])
axs[1].set_title("Test Data")
plt.plot()
plt.show()
"""From the figure, it is clear that what data-points are anomaly. However, this is just for test case. Yellow points (class 1) are out anomalies."""
# Visualizing one of the Isolation Tree
_ = tree.plot_tree(mdl.estimators_[0])
# Fit Isolation Forest Model
"""By default, the model will have 100 tress and the subsample size is 256.
There is another parameter call contamination rate, which is percentages of
anomalies supposed to present in the dataset."""
mdl = IsolationForest(random_state=112233, contamination=0.02).fit(X_train)
# Visualizing one of the Isolation Tree
_ = tree.plot_tree(mdl.estimators_[0])
# Visualizing one of the Isolation Tree
_ = tree.plot_tree(mdl.estimators_[0])
plt.show()
# Visualizing one of the Isolation Tree
_ = tree.plot_tree(mdl.estimators_[0])
_.show()
# Visulize the Train & Test data
fig, axs = plt.subplots(1,2)
axs[0].scatter(x=X_train['X1'], y=X_train['X2'], c=y_train['target'],)
axs[0].set_title("Train Data")
axs[1].scatter(x=X_test['X1'], y=X_test['X2'], c=y_test['target'])
axs[1].set_title("Test Data")
plt.plot()
plt.show()
"""From the figure, it is clear that what data-points are anomaly. However, this
is just for test case. Yellow points (class 1) are out anomalies."""
# Visualizing one of the Isolation Tree
_ = tree.plot_tree(mdl.estimators_[0])
plt.plot()
# Visualizing one of the Isolation Tree
_ = tree.plot_tree(mdl.estimators_[0])
plt.plot()
plt.show()
reticulate::repl_python()
# Libraries Import
from sklearn.datasets import make_blobs, make_moons
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import IsolationForest
from sklearn.metrics import accuracy_score
from sklearn import tree
from matplotlib import pyplot as plt
import warnings
warnings.filterwarnings('ignore')
# Dummy dataset Generation
data = make_blobs(centers=[[0,0],[2,2]],
cluster_std=0.4,
n_samples=[350,10],
n_features=2,
random_state=112233)
# Features of the 'data'
feat = pd.DataFrame(data[0], columns=['X1', 'X2'])
# Target column of the 'data'
target = pd.DataFrame(data[1], columns=['target'])
# Combine Both to make a data-frame
df = pd.concat([feat, target], axis=1)
print(df['target'].value_counts())
"""There are total 360 data-points, out of which 350 are of class 0 and 10 of class 1. Also, there are 2 continuous features names as X1 and X2. As mentioned previously, even anomaly detection is majorly unsupervised, for the sake of comparision we are derriving as supervised learning"""
# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(feat, target, train_size=0.7,
random_state=112233,
stratify=target,
shuffle=True)
# Visulize the Train & Test data
fig, axs = plt.subplots(1,2)
axs[0].scatter(x=X_train['X1'], y=X_train['X2'], c=y_train['target'],)
axs[0].set_title("Train Data")
axs[1].scatter(x=X_test['X1'], y=X_test['X2'], c=y_test['target'])
axs[1].set_title("Test Data")
plt.plot()
plt.show()
"""From the figure, it is clear that what data-points are anomaly. However, this
is just for test case. Yellow points (class 1) are out anomalies."""
# Fit Isolation Forest Model
"""By default, the model will have 100 tress and the subsample size is 256.
There is another parameter call contamination rate, which is percentages of
anomalies supposed to present in the dataset."""
mdl = IsolationForest(random_state=112233, contamination=0.02).fit(X_train)
# Visualizing one of the Isolation Tree
fig, ax = plt.subplots(1,1)
_ = tree.plot_tree(ax = ax[0], mdl.estimators_[0])
plt.plot()
plt.show()
# Visualizing one of the Isolation Tree
fig, ax = plt.subplots(1,1)
_ = tree.plot_tree(ax = ax[1], mdl.estimators_[0])
plt.plot()
plt.show()
# Visualizing one of the Isolation Tree
#fig, ax = plt.subplots(1,1)
_ = tree.plot_tree(mdl.estimators_[0])
plt.plot()
plt.show()
# Prediction on the Test Data
pred = mdl.predict(X_test)
"""The Predict function outputs the values as 1 and -1. -1 being anomalies.
We need to convert this back to 0 and 1. For us the class 1 are anomalies"""
# Decode to it original format
pred_decode = [1 if i==-1 else 0 for i in pred]
# Accuracy
print(f"The accuracy is: {accuracy_score(y_test, pred_decode)}")
"""We got the accuracy of 100% on the test data, which is what I was expecting, as the data was easily sepearatable.
However, the purpose of the demo is to understand how to use algorithm with sklearn. You will face complex situation in real-world"""
# Visualizing one of the Isolation Tree
#fig, ax = plt.subplots(1,1)
_ = tree.plot_tree(mdl.estimators_[0])
plt.plot()
plt.show()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown::stop_server()
blogdown::stop_server()
blogdown::stop_server()
install.packages("blogdown")
install.packages("blogdown")
install.packages("blogdown")
install.packages("blogdown")
install.packages("blogdown")
blogdown:::serve_site()
library(abind)
blogdown::install_hugo("0.81.0")
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
* Kafka
- title: Data Integration Engineer
- title: Data Integration Engineer
- title: Data Integration Engineer
title: Data Integration Engineer
- title: Data Integration Engineer
title: Data Integration Engineer
title: Data Integration Engineer
blogdown::stop_server()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
blogdown::stop_server()
blogdown:::serve_site()
blogdown::stop_server()
library(xfun)
install.packages("xfun")
library(xfun)
install.packages("xfun")
install.packages("xfun")
install.packages("xfun")
install.packages("xfun")
library(xfun)
detach("package:xfun", unload = TRUE)
library(xfun)
install.packages("xfun")
library(xfun)
detach("package:xfun", unload = TRUE)
install.packages(c("agricolae", "askpass", "broom", "bslib", "cachem", "classInt", "cpp11", "curl", "dbplyr", "digest", "dplyr", "DT", "ellipse", "emmeans", "evaluate", "fontawesome", "fs", "gargle", "ggplot2", "googledrive", "googlesheets4", "gtable", "haven", "htmltools", "httpuv", "httr", "jsonlite", "knitr", "labeling", "labelled", "later", "lme4", "markdown", "MatrixModels", "minqa", "multcompView", "mvtnorm", "openssl", "pkgload", "processx", "promises", "ps", "purrr", "quantreg", "Rcpp", "readxl", "rematch", "rlang", "rmarkdown", "rstudioapi", "sass", "scatterplot3d", "shiny", "styler", "sys", "testthat", "tinytex", "tzdb", "uuid", "vctrs", "viridis", "viridisLite", "vroom", "waldo", "xfun", "xml2"))
install.packages(c("agricolae", "askpass", "broom", "bslib", "cachem", "classInt", "cpp11", "curl", "dbplyr", "digest", "dplyr", "DT", "ellipse", "emmeans", "evaluate", "fontawesome", "fs", "gargle", "ggplot2", "googledrive", "googlesheets4", "gtable", "haven", "htmltools", "httpuv", "httr", "jsonlite", "knitr", "labeling", "labelled", "later", "lme4", "markdown", "MatrixModels", "minqa", "multcompView", "mvtnorm", "openssl", "pkgload", "processx", "promises", "ps", "purrr", "quantreg", "Rcpp", "readxl", "rematch", "rlang", "rmarkdown", "rstudioapi", "sass", "scatterplot3d", "shiny", "styler", "sys", "testthat", "tinytex", "tzdb", "uuid", "vctrs", "viridis", "viridisLite", "vroom", "waldo", "xfun", "xml2"))
install.packages(c("agricolae", "askpass", "broom", "bslib", "cachem", "classInt", "cpp11", "curl", "dbplyr", "digest", "dplyr", "DT", "ellipse", "emmeans", "evaluate", "fontawesome", "fs", "gargle", "ggplot2", "googledrive", "googlesheets4", "gtable", "haven", "htmltools", "httpuv", "httr", "jsonlite", "knitr", "labeling", "labelled", "later", "lme4", "markdown", "MatrixModels", "minqa", "multcompView", "mvtnorm", "openssl", "pkgload", "processx", "promises", "ps", "purrr", "quantreg", "Rcpp", "readxl", "rematch", "rlang", "rmarkdown", "rstudioapi", "sass", "scatterplot3d", "shiny", "styler", "sys", "testthat", "tinytex", "tzdb", "uuid", "vctrs", "viridis", "viridisLite", "vroom", "waldo", "xfun", "xml2"))
library(xfun)
install.packages("xfun")
detach("package:xfun", unload = TRUE)
library(xfun)
detach("package:xfun", unload = TRUE)
install.packages("xfun")
library(xfun)
install.packages("xfun")
install.packages("xfun")
detach("package:xfun", unload = TRUE)
install.packages("xfun")
install.packages("xfun")
blogdown:::preview_site()
blogdown::stop_server()
blogdown::serve_site()
